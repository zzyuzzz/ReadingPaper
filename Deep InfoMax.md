# LEARNING DEEP REPRESENTATIONS BY MUTUAL INFORMATION ESTIMATION AND MAXIMIZATION(通过互信息估计和最大化来学习深度表示)

这项工作通过最大化深度神经网络编码器的输入和输出之间的互信息来研究表示的无监督学习。重要的是，我们表明结构很重要：将输入中的局部性知识纳入目标可以显着提高表示对下游任务的适用性。我们通过对抗性地匹配先验分布来进一步控制表示的特征。我们的方法被称为 Deep InfoMax (DIM)，它的性能优于许多流行的无监督学习方法，并且在某些标准架构的多个分类任务上与完全监督学习相媲美。 DIM 为表征的无监督学习开辟了新的途径，并且是朝着针对特定最终目标灵活制定表征学习目标的重要一步。

深度学习的一个核心目标是发现有用的表示，这里探讨的简单想法是训练表示学习函数，即编码器，以最大化其输入和输出之间的互信息（MI）。众所周知，MI 很难计算，尤其是在连续和高维设置中。幸运的是，最近的进展使得深度神经网络的高维输入/输出对之间的 MI 能够有效计算（Belghazi 等人，2018）。我们利用 MI 估计进行表示学习，并表明，根据下游任务，最大化完整输入和编码器输出之间的 MI（即全局 MI）通常不足以学习有用的表示。相反，结构很重要：最大化输入的表示和局部区域（例如补丁而不是完整图像）之间的平均 MI 可以极大地提高表示的质量，例如分类任务，而全局 MI 在能力中发挥更重要的作用重建给定表示的完整输入。

表征的有用性不仅仅是信息内容的问题：独立性等表征特征也发挥着重要作用（Gretton et al., 2012; Hyv ̈ arinen & Oja, 2000; Hinton, 2002; Schmidhuber, 1992; Bengio et al., 2012）。 ，2013；托马斯等人，2017）。我们以类似于对抗性自动编码器（AAE，Makhzani 等人，2015）的方式将 MI 最大化与先验匹配相结合，以根据所需的统计属性来约束表示。这种方法与 infomax 优化原理密切相关（Linsker，1988；Bell & Sejnowski，1995），因此我们将我们的方法称为 Deep InfoMax（DIM）。我们的主要贡献如下：

1. 我们形式化了 Deep InfoMax (DIM)，它同时估计并最大化输入数据和学习的高级表示之间的互信息。
2. 我们的互信息最大化过程可以优先考虑全局或局部信息，我们表明这些信息可用于调整学习表示对分类或重建类型任务的适用性。
3. 我们使用对抗性学习（`a la Makhzani et al., 2015）来约束表示，使其具有特定于先验的所需统计特征。
4. 我们引入了两种新的表示质量度量，一种基于互信息神经估计（MINE，Belghazi 等人，2018 年），另一种基于 Brakel 和 Bengio（2017 年）的工作的神经依赖度量（NDM），我们使用这些度量以支持我们将 DIM 与不同的无监督方法进行比较。

## 相关工作
有许多流行的学习表示的方法。经典方法，例如独立成分分析（ICA，Bell＆Sejnowski，1995）和自组织图（Kohonen，1998），通常缺乏深度神经网络的表示能力。最近的方法包括深度体积保留图（Dinh 等人，2014 年；2016 年）、深度聚类（Xie 等人，2016 年；Chang 等人，2017 年）、噪声作为目标（NAT、Bojanowski 和 Joulin，2017 年） ，以及自我监督或共同学习（Doersch & Zisserman，2017；Dosovitskiy 等人，2016；Sajjadi 等人，2016）。