# MoE：混合专家模型介绍（一）

本文是对[混合专家模型 (MoE) 详解](https://huggingface.co/blog/zh/moe)重点摘要与归纳，有兴趣的小伙伴可以点击链接阅读原文。

## 混合专家模型 (MoEs)特点
+ 与稠密模型相比，**预训练速度更快**
+ 与具有相同参数数量的模型相比，具有**更快的推理速度**
+ 需要**大量显存**，因为所有专家系统都需要加载到内存中
+ 在**微调方面存在诸多挑战**，但近期的研究 表明，对混合专家模型进行**指令调优具有很大的潜力**。

## 什么是混合专家模型？

作为一种基于 Transformer 架构的模型，混合专家模型主要由两个关键部分组成:
+ **稀疏 MoE 层**:这些层代替了传统 Transformer 模型中的前馈网络 (FFN) 层。通过多个称为“专家”的独立神经网络(一般是FFN，当然也可以是MoE本身)的选择性激活，提高模型效率与灵活性。
+ **门控网络或路由**: 这个部分用于决定哪些token被发送到哪个专家。一个令牌可以被发送到多个专家。令牌的路由方式是 MoE 使用中的一个关键点，因为路由器(router)由学习的参数组成，并且与网络的其他部分一同进行预训练。

![](picture/00_switch_transformer.png)

### 遭遇的挑战
+ **训练挑战:** 在**微调阶段**往往面临**泛化能力不足**的问题，长期以来易于引发过拟合现象。
+ **推理挑战:** MoE模型在**推理过程中只使用其大量参数中的一部分**，这使得它们的推理速度快于具有相同数量参数的稠密模型。然而，这种模型需要将所有参数加载到内存中，因此对**内存的需求非常高**。